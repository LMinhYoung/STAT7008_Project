{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "nltk.data.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cepak', 'saka', 'hotelku', 'nginep', ',', 'namung', 'digawa', 'mlaku', ',', 'ing', 'kene', 'akeh', 'tenan', 'pilian', 'panganane', ',', 'panggonane', 'sing', 'amba', ',', 'lan', 'nyenengake']\n",
      "['Nikmati', 'cicilan', '0', '%', 'hingga', '12', 'bulan', 'untuk', 'pemesanan', 'tiket', 'pesawat', 'air', 'asia', 'dengan', 'kartu', 'kredit', 'bni', '!']\n",
      "['Kue-kue', 'yang', 'disajikan', 'bikin', 'saya', 'bernostalgia', '.', 'Semuanya', 'tipikal', 'kue', 'zaman', 'dulu', ',', 'baik', 'dari', 'penampilan', 'maupun', 'rasa', '.', 'Kuenya', 'enak', 'dan', 'harganya', 'juga', 'murah', '.']\n",
      "['Yeah', 'that', \"'s\", 'right', ',', 'he', \"'s\", 'looking', 'after', 'the', 'store', 'now']\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Cepak saka hotelku nginep, namung digawa mlaku, ing kene akeh tenan pilian panganane, panggonane sing amba, lan nyenengake\",\n",
    "         'Nikmati cicilan 0% hingga 12 bulan untuk pemesanan tiket pesawat air asia dengan kartu kredit bni!',\n",
    "         'Kue-kue yang disajikan bikin saya bernostalgia. Semuanya tipikal kue zaman dulu, baik dari penampilan maupun rasa. Kuenya enak dan harganya juga murah.',\n",
    "         \"Yeah that's right, he's looking after the store now\"]\n",
    "for text in texts:\n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 使用 PyTorch 填充\u001b[39;00m\n\u001b[1;32m     20\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(\n\u001b[0;32m---> 21\u001b[0m     [torch\u001b[38;5;241m.\u001b[39mtensor(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m tokens],\n\u001b[1;32m     22\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 打印填充结果\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadded Sequences using PyTorch:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 使用 PyTorch 填充\u001b[39;00m\n\u001b[1;32m     20\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(\n\u001b[0;32m---> 21\u001b[0m     [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m tokens],\n\u001b[1;32m     22\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 打印填充结果\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadded Sequences using PyTorch:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 示例数据\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is the first sentence.\",\n",
    "        \"Here is the second one.\",\n",
    "        \"And this is the third sentence.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 分词\n",
    "tokens = [word_tokenize(text) for text in data['text']]\n",
    "\n",
    "# 确定最大长度\n",
    "max_length = max(len(t) for t in tokens)\n",
    "\n",
    "# 使用 PyTorch 填充\n",
    "padded_sequences = torch.nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(seq) for seq in tokens],\n",
    "    batch_first=True,\n",
    "    padding_value=0\n",
    ")\n",
    "\n",
    "# 打印填充结果\n",
    "print(\"Padded Sequences using PyTorch:\")\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0000e+09)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "target = torch.tensor([0,2,1])\n",
    "input = torch.tensor([[5.0,6.0,7.0],\n",
    "                      [3.0,2.0,-1e10],\n",
    "                      [-10.0,1.0,7.0]])\n",
    "loss_item = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = loss_item(input, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "target = torch.tensor([0,2,1])\n",
    "input = torch.tensor([[1e10,-1e10,7.0],\n",
    "                      [3.0,2.0,1.0],\n",
    "                      [-10.0,1.0,7.0]])\n",
    "loss_item = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = loss_item(input, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在你的代码中，尽管你使用了 ignore_index=0，\n",
    "# 但仍然得到一个很大的损失值（tensor(5.0000e+09)）。\n",
    "# 这是因为 ignore_index 只会在计算损失时忽略目标标签中为 0 的样本，\n",
    "# 而不会直接忽略 logits 中对应的值。以下是更详细的解释："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_manual = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
